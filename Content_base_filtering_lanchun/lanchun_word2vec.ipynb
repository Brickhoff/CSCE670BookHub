{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28903\n",
      "{'book_id': '16037549', 'description': 'number 30 in a series of literary pamphlets published monthly and available at the price of 15 cents per copy  or a yearly subscription  19 numbers  for $1 25'}\n"
     ]
    }
   ],
   "source": [
    "#load preprocessed data\n",
    "import json\n",
    "import math\n",
    "\n",
    "from pprint import pprint\n",
    "data = []\n",
    "with open('poetry.json') as data_file:\n",
    "    for line in data_file:\n",
    "        data.append(json.loads(line))\n",
    "print(len(data))        \n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['number', '30', 'in', 'a', 'series', 'of', 'literary', 'pamphlets', 'published', 'monthly', 'and', 'available', 'at', 'the', 'price', 'of', '15', 'cents', 'per', 'copy', 'or', 'a', 'yearly', 'subscription', '19', 'numbers', 'for', '$1', '25']\n"
     ]
    }
   ],
   "source": [
    "#tokenized descriptions of each books\n",
    "size_data = len(data)   \n",
    "\n",
    "list_description = []\n",
    "list_word = []\n",
    "strings = [0]*len(data)\n",
    "\n",
    "for index in range(len(data)):\n",
    "    list_description.append(data[index]['description'].split())\n",
    "print(list_description[0])\n",
    "\n",
    "#for e in list_description[0]:\n",
    " #   print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#load glove word vector\n",
    "import numpy as np\n",
    "\n",
    "with open(\"glove.6B.50d.txt\", \"rb\") as lines:\n",
    "    model = {line.split()[0]: np.array(list(map(float, line.split()[1:])))\n",
    "           for line in lines}\n",
    "    \n",
    "print(model[b'the'])\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector representation starts\n",
      "[0.04462296428571428, 0.41778628571428567, 0.17888371428571426, -0.14154392857142858, 0.3240271071428572, 0.1980658571428571, -0.5824249999999999, -0.5412649642857144, 0.10414792035714285, -0.01185017857142856, -0.10247546428571429, -0.2655082142857143, 0.03153778571428572, -0.28545675, 0.6473482321428572, -0.2985690714285714, -0.3826425714285714, -0.14898582857142859, -0.6632431428571428, -0.17489567857142857, 0.6522740714285714, -0.1255387142857143, 0.40797942857142855, 0.006020250000000014, -0.2409779392857143, -0.991709492857143, -0.27011017857142866, -0.27843142857142855, -0.009619857142857137, 0.10061675000000002, 3.2126607142857146, 0.0005683214285714348, -0.020838500000000003, -0.028826964285714303, 0.20331436178571427, -0.09441306428571429, 0.21102067857142862, 0.11205085714285715, 0.08838549999999999, -0.06415946428571428, 0.12319985714285711, -0.022866521428571422, 0.07736035714285716, 0.11040539285714283, -0.3666960714285715, -0.028651408928571424, -0.05260603928571429, 0.07191432142857145, 0.04706335714285714, -0.05251128571428571]\n",
      "vector representation is done\n"
     ]
    }
   ],
   "source": [
    "#calculate vector representation for each descrption by meaning the word vectors\n",
    "print('vector representation starts')\n",
    "size_data = len(data)   \n",
    "array_description = []#each decription can be displayed as a array with length 50\n",
    "       \n",
    "for index in range(size_data):#a descrption\n",
    "    sum = [0]*50\n",
    "    count = 0\n",
    "    for e in data[index]['description'].split():# a word in review\n",
    "        byte_e=e.lower().encode()\n",
    "        if(byte_e in model):#valid word\n",
    "            #print(e)\n",
    "            for i in range(50):#valid array can be summed\n",
    "                sum[i] = sum[i]+model[byte_e][i]\n",
    "            #print(sum[0])\n",
    "            count = count + 1\n",
    "            #print(count)\n",
    "    if(count != 0):\n",
    "        for num in range(50):\n",
    "            sum[num] = sum[num]/count\n",
    "    array_description.append(sum)\n",
    "    #print(sum[0])\n",
    "print(array_description[0])\n",
    "              \n",
    "print('vector representation is done')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28903\n",
      "16037549\n"
     ]
    }
   ],
   "source": [
    "#create list for book id\n",
    "list_bookid = []\n",
    "print(len(array_description))\n",
    "for index in range(len(data)):\n",
    "    list_bookid.append(data[index]['book_id'])\n",
    "print(list_bookid[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28903\n",
      "{'book_id': '16037549', 'description': [0.04462296428571428, 0.41778628571428567, 0.17888371428571426, -0.14154392857142858, 0.3240271071428572, 0.1980658571428571, -0.5824249999999999, -0.5412649642857144, 0.10414792035714285, -0.01185017857142856, -0.10247546428571429, -0.2655082142857143, 0.03153778571428572, -0.28545675, 0.6473482321428572, -0.2985690714285714, -0.3826425714285714, -0.14898582857142859, -0.6632431428571428, -0.17489567857142857, 0.6522740714285714, -0.1255387142857143, 0.40797942857142855, 0.006020250000000014, -0.2409779392857143, -0.991709492857143, -0.27011017857142866, -0.27843142857142855, -0.009619857142857137, 0.10061675000000002, 3.2126607142857146, 0.0005683214285714348, -0.020838500000000003, -0.028826964285714303, 0.20331436178571427, -0.09441306428571429, 0.21102067857142862, 0.11205085714285715, 0.08838549999999999, -0.06415946428571428, 0.12319985714285711, -0.022866521428571422, 0.07736035714285716, 0.11040539285714283, -0.3666960714285715, -0.028651408928571424, -0.05260603928571429, 0.07191432142857145, 0.04706335714285714, -0.05251128571428571]}\n",
      "28903\n"
     ]
    }
   ],
   "source": [
    "#combin book id and description as a dictionary for each book\n",
    "print(len(list_bookid))\n",
    "data_id_matrix = []\n",
    "for e in range(len(list_bookid)):\n",
    "    data_id_matrix.append({})\n",
    "for e in range(len(data)):\n",
    "    data_id_matrix[e]['book_id']=list_bookid[e]\n",
    "    data_id_matrix[e]['description']=array_description[e]\n",
    "print(data_id_matrix[0])\n",
    "\n",
    "print(len(data_id_matrix))\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the dictionary into drive\n",
    "import json\n",
    "\n",
    "fileObject = open('final_data.json', 'w') #backup copy for dictionary with id and matrix\n",
    "for e in range(len(data)):\n",
    "    fileObject.write(json.dumps(data_id_matrix[e]))\n",
    "    fileObject.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drive from the local data\n",
    "#import json\n",
    "#import math\n",
    "#from pprint import pprint\n",
    "#data_id_matrix = []\n",
    "#with open('raw_data_id_matrix.json') as data_file:\n",
    "#    for line in data_file:\n",
    "#        data_id_matrix.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------similarity rank--------\n",
      "\n",
      "    book_id    similarity(Word2Vec)\n",
      "1  31117220  1.0000000000000002 \n",
      "2  26206414  1.0000000000000002 \n",
      "3  25330489  1.0000000000000002 \n",
      "4  27169927  0.9969203549118102 \n",
      "5  653046  0.988487307829749 \n",
      "6  36944  0.9877275062421972 \n",
      "7  661721  0.9876225861775914 \n",
      "8  22113280  0.9874620291919007 \n",
      "9  22204746  0.9874620291919007 \n",
      "10  8264228  0.9872596466268453 \n",
      "11  6419933  0.9867536391191153 \n"
     ]
    }
   ],
   "source": [
    "#print 10 books which has the highest similarity with the target book\n",
    "def takeSecond(elem):\n",
    "        return elem[1]\n",
    "\n",
    "def similarity (book_id):#given a book_id of target book, return 10 book which has the highest similarity with that \n",
    "    list_sim = [] #similarity with the target book\n",
    "    index = list_bookid.index(book_id)\n",
    "    matrix = data_id_matrix[index]['description']# matrix of target book \n",
    "    \n",
    "    divider_target=0\n",
    "    for num in range(50):\n",
    "        divider_target = pow(matrix[num],2) + divider_target\n",
    "    divider_target = math.sqrt(divider_target)    \n",
    "    \n",
    "    for e in range(len(data_id_matrix)):#each book\n",
    "        numerator = 0\n",
    "        divider = 0\n",
    "        cos = 0\n",
    "        for i in range(50):\n",
    "            numerator = numerator + matrix[i]*data_id_matrix[e]['description'][i]\n",
    "            divider = divider + pow(data_id_matrix[e]['description'][i],2)\n",
    "        divider = math.sqrt(divider) \n",
    "        if(divider != 0 and divider_target!=0):\n",
    "            cos = numerator/(divider*divider_target)\n",
    "        list_sim.append(cos)\n",
    "    #print(list_sim[0])  \n",
    "    #print(list_sim[1]) \n",
    "    \n",
    "    #pair the similarity and the book, rank the similarity\n",
    "    list_pair=[]\n",
    "    pair= zip(list_bookid, list_sim)\n",
    "    for e in pair:\n",
    "        list_pair.append(e)    \n",
    "\n",
    "    list_pair.sort(key= takeSecond, reverse=True)\n",
    "    #print(list_pair[0])\n",
    "    \n",
    "    print('-------similarity rank--------')\n",
    "    print('')\n",
    "    print('    book_id   ','similarity(Word2Vec)' )\n",
    "    string = [0]*len(list_pair)\n",
    "    for index in range(len(list_pair)):\n",
    "        string[index]= str(list_pair[index]).replace(\")\", ' ')\n",
    "        string[index]= string[index].replace(\"(\", ' ')\n",
    "        string[index]= string[index].replace(\"'\", '')\n",
    "        string[index]= string[index].replace(\",\", ' ')\n",
    "        print(index+1, string[index])\n",
    "        if index == 10:\n",
    "            break\n",
    "similarity('25330489')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
